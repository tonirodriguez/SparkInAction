{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1381634",
   "metadata": {},
   "source": [
    "#Â Spark in Action - Chapter 3 Scala Version - Lab 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bba815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.41:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1667056989974)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, concat, lit, split}\n",
       "import org.apache.spark.sql.{Dataset, Row, SparkSession}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, concat, lit, split}\n",
    "import org.apache.spark.sql.{Dataset, Row, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a61b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 17:23:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@35aab3d4\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creates a session on a local master\n",
    "val spark = SparkSession\n",
    "                .builder\n",
    "                .appName(\"Union of two dataframes\")\n",
    "                .master(\"local[*]\")\n",
    "                .getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f247e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [OBJECTID: string, HSISID: string ... 13 more fields]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df1 = spark\n",
    "            .read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(\"../net.jgp.books.spark.ch03/data/Restaurants_in_Wake_County_NC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [datasetid: string, fields: struct<closing_date: string, est_group_desc: string ... 21 more fields> ... 3 more fields]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df2 = spark\n",
    "            .read\n",
    "            .format(\"json\")\n",
    "            .load(\"../net.jgp.books.spark.ch03/data/Restaurants_in_Durham_County_NC.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7af2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wakeRestaurantsDf: org.apache.spark.sql.DataFrame = [datasetId: string, name: string ... 13 more fields]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "* Builds the dataframe containing the Wake county restaurants\n",
    "*\n",
    "* @return A dataframe\n",
    "*/\n",
    "private def buildWakeRestaurantsDataframe(df: Dataset[Row]) = {\n",
    "    val drop_cols = List(\"OBJECTID\", \"GEOCODESTATUS\", \"PERMITID\")\n",
    "    var df1 = df.withColumn(\"county\", lit(\"Wake\"))\n",
    "                .withColumnRenamed(\"HSISID\", \"datasetId\")\n",
    "                .withColumnRenamed(\"NAME\", \"name\")\n",
    "                .withColumnRenamed(\"ADDRESS1\", \"address1\")\n",
    "                .withColumnRenamed(\"ADDRESS2\", \"address2\")\n",
    "                .withColumnRenamed(\"CITY\", \"city\")\n",
    "                .withColumnRenamed(\"STATE\", \"state\")\n",
    "                .withColumnRenamed(\"POSTALCODE\", \"zip\")\n",
    "                .withColumnRenamed(\"PHONENUMBER\", \"tel\")\n",
    "                .withColumnRenamed(\"RESTAURANTOPENDATE\", \"dateStart\")\n",
    "                .withColumn(\"dateEnd\", lit(null))\n",
    "                .withColumnRenamed(\"FACILITYTYPE\", \"type\")\n",
    "                .withColumnRenamed(\"X\", \"geoX\")\n",
    "                .withColumnRenamed(\"Y\", \"geoY\")\n",
    "                .drop(drop_cols:_*)\n",
    "\n",
    "    df1 = df1.withColumn(\"id\",\n",
    "             concat(col(\"state\"), lit(\"_\"), col(\"county\"), lit(\"_\"), col(\"datasetId\")))\n",
    "    // I left the following line if you want to play with repartitioning\n",
    "    // df1 = df1.repartition(4);\n",
    "    df1\n",
    "}\n",
    "\n",
    "val wakeRestaurantsDf = buildWakeRestaurantsDataframe(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e851c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "durhamRestaurantsDf: org.apache.spark.sql.DataFrame = [datasetId: string, county: string ... 13 more fields]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "* Builds the dataframe containing the Durham county restaurants\n",
    "*\n",
    "* @return A dataframe\n",
    "*/\n",
    "private def buildDurhamRestaurantsDataframe(df: Dataset[Row]) = {\n",
    "    val drop_cols=List(\"fields\", \"geometry\", \"record_timestamp\", \"recordid\")\n",
    "    var df1 = df.withColumn(\"county\", lit(\"Durham\"))\n",
    "                .withColumn(\"datasetId\", col(\"fields.id\"))\n",
    "                .withColumn(\"name\", col(\"fields.premise_name\"))\n",
    "                .withColumn(\"address1\", col(\"fields.premise_address1\"))\n",
    "                .withColumn(\"address2\", col(\"fields.premise_address2\"))\n",
    "                .withColumn(\"city\", col(\"fields.premise_city\"))\n",
    "                .withColumn(\"state\", col(\"fields.premise_state\"))\n",
    "                .withColumn(\"zip\", col(\"fields.premise_zip\"))\n",
    "                .withColumn(\"tel\", col(\"fields.premise_phone\"))\n",
    "                .withColumn(\"dateStart\", col(\"fields.opening_date\"))\n",
    "                .withColumn(\"dateEnd\", col(\"fields.closing_date\"))\n",
    "                .withColumn(\"type\", split(col(\"fields.type_description\"), \" - \").getItem(1))\n",
    "                .withColumn(\"geoX\", col(\"fields.geolocation\").getItem(0))\n",
    "                .withColumn(\"geoY\", col(\"fields.geolocation\").getItem(1))\n",
    "                .drop(drop_cols:_*)\n",
    "\n",
    "    df1 = df1.withColumn(\"id\",\n",
    "                  concat(col(\"state\"), lit(\"_\"), col(\"county\"), lit(\"_\"), col(\"datasetId\")))\n",
    "    // I left the following line if you want to play with repartitioning\n",
    "    // df1 = df1.repartition(4);\n",
    "    df1\n",
    "}\n",
    "\n",
    "val durhamRestaurantsDf = buildDurhamRestaurantsDataframe(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b3660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetId: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- tel: string (nullable = true)\n",
      " |-- dateStart: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- geoX: string (nullable = true)\n",
      " |-- geoY: string (nullable = true)\n",
      " |-- county: string (nullable = false)\n",
      " |-- dateEnd: void (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wakeRestaurantsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f228687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetId: string (nullable = true)\n",
      " |-- county: string (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- tel: string (nullable = true)\n",
      " |-- dateStart: string (nullable = true)\n",
      " |-- dateEnd: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- geoX: double (nullable = true)\n",
      " |-- geoY: double (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "durhamRestaurantsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a231b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combineDataframes: (df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])Unit\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "* Performs the union between the two dataframes.\n",
    "*\n",
    "* @param df1 Left Dataframe to union on\n",
    "* @param df2 Right Dataframe to union from\n",
    "*/\n",
    "def combineDataframes(df1: Dataset[Row], df2: Dataset[Row]): Unit = {\n",
    "    val df = df1.unionByName(df2)\n",
    "    df.show(5)\n",
    "    df.printSchema()\n",
    "    println(\"We have \" + df.count + \" records.\")\n",
    "    val partitionCount = df.rdd.getNumPartitions\n",
    "    println(\"Partition count: \" + partitionCount)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+--------------------+-----------------+------------+-----------+------+-------+-------------------+\n",
      "|  datasetId|                name|            address1|address2|       city|state|       zip|           tel|           dateStart|             type|        geoX|       geoY|county|dateEnd|                 id|\n",
      "+-----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+--------------------+-----------------+------------+-----------+------+-------+-------------------+\n",
      "|04092016024|                WABA|2502 1/2 HILLSBOR...|    null|    RALEIGH|   NC|     27607|(919) 833-1710|2011-10-18T00:00:...|       Restaurant|-78.66818477|35.78783803|  Wake|   null|NC_Wake_04092016024|\n",
      "|04092021693|  WALMART DELI #2247|2010 KILDAIRE FAR...|    null|       CARY|   NC|     27518|(919) 852-6651|2011-11-08T00:00:...|       Food Stand|-78.78211173|35.73717591|  Wake|   null|NC_Wake_04092021693|\n",
      "|04092017012|CAROLINA SUSHI &a...|5951-107 POYNER V...|    null|    RALEIGH|   NC|     27616|(919) 981-5835|2015-08-28T00:00:...|       Restaurant|-78.57030208|35.86511564|  Wake|   null|NC_Wake_04092017012|\n",
      "|04092030288|THE CORNER VENEZU...|    7500 RAMBLE WAY |    null|    RALEIGH|   NC|     27616|          null|2015-09-04T00:00:...|Mobile Food Units|  -78.537511|35.87630712|  Wake|   null|NC_Wake_04092030288|\n",
      "|04092015530|        SUBWAY #3726| 12233 CAPITAL BLVD |    null|WAKE FOREST|   NC|27587-6200|(919) 556-8266|2009-12-11T00:00:...|       Restaurant|-78.54097555|35.98087357|  Wake|   null|NC_Wake_04092015530|\n",
      "+-----------+--------------------+--------------------+--------+-----------+-----+----------+--------------+--------------------+-----------------+------------+-----------+------+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- datasetId: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- tel: string (nullable = true)\n",
      " |-- dateStart: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- geoX: string (nullable = true)\n",
      " |-- geoY: string (nullable = true)\n",
      " |-- county: string (nullable = false)\n",
      " |-- dateEnd: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n",
      "We have 5903 records.\n",
      "Partition count: 2\n"
     ]
    }
   ],
   "source": [
    "combineDataframes(wakeRestaurantsDf, durhamRestaurantsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a597883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
