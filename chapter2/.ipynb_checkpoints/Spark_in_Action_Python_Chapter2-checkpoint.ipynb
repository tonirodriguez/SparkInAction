{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b64a4e",
   "metadata": {},
   "source": [
    "# Spark in Action - Chapter 2 Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18daac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() #os.path.dirname(__file__)\n",
    "relative_path = \"../net.jgp.books.spark.ch02/data/authors.csv\"\n",
    "absolute_file_path = os.path.join(current_dir, relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498fa42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/development/ml/Spark/chapter2/../net.jgp.books.spark.ch02/data/authors.csv'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 17:17:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Creates a session on a local master\n",
    "spark = SparkSession.builder.appName(\"CSV to DB\").master(\"local\").config(\"spark.jars\",\"{}/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd())).config(\"spark.driver.extraClassPath\",\"{}/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd())).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#  Step 1: Ingestion\n",
    "#  ---------\n",
    "#\n",
    "#  Reads a CSV file with header, called authors.csv, stores it in a dataframe\n",
    "df = spark.read.csv(header=True, inferSchema=True, path=absolute_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0759ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|   lname|         fname|\n",
      "+--------+--------------+\n",
      "|  Pascal|        Blaise|\n",
      "|Voltaire|      François|\n",
      "|  Perrin|  Jean-Georges|\n",
      "|Maréchal|Pierre Sylvain|\n",
      "|   Karau|        Holden|\n",
      "| Zaharia|         Matei|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transform\n",
    "# ---------\n",
    "# Creates a new column called \"name\" as the concatenation of lname, a\n",
    "# virtual column containing \", \" and the fname column\n",
    "df = df.withColumn(\"name\", F.concat(F.col(\"lname\"), F.lit(\", \"), F.col(\"fname\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ea3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+\n",
      "|   lname|         fname|                name|\n",
      "+--------+--------------+--------------------+\n",
      "|  Pascal|        Blaise|      Pascal, Blaise|\n",
      "|Voltaire|      François|  Voltaire, François|\n",
      "|  Perrin|  Jean-Georges|Perrin, Jean-Georges|\n",
      "|Maréchal|Pierre Sylvain|Maréchal, Pierre ...|\n",
      "|   Karau|        Holden|       Karau, Holden|\n",
      "| Zaharia|         Matei|      Zaharia, Matei|\n",
      "+--------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99deed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Save\n",
    "# ----\n",
    "#\n",
    "# The connection URL, assuming your PostgreSQL instance runs locally on the\n",
    "# default port, and the database we use is \"spark_labs\"\n",
    "dbConnectionUrl = \"jdbc:sqlite:/Users/development/ml/Spark/net.jgp.books.spark.ch02/data/spark_labs.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02facd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties to connect to the database, the JDBC driver is part of our pom.xml\n",
    "prop = {\"driver\":\"org.sqlite.JDBC\", \"user\":\"jgp\", \"password\":\"Spark<3Java\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 17:17:47 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    }
   ],
   "source": [
    "# Write in a table called ch02\n",
    "df.write.mode(\"overwrite\").jdbc(url=dbConnectionUrl, table=\"ch02\", properties=prop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good to stop SparkSession at the end of the application\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5855a",
   "metadata": {},
   "source": [
    "## Método alternativo según Manual Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304fa4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect('example.db')\n",
    "cur = con.cursor()\n",
    "# Create table\n",
    "cur.execute(\n",
    "    '''CREATE TABLE stocks\n",
    "       (date text, trans text, symbol text, qty real, price real)''')\n",
    "# Insert a row of data\n",
    "cur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n",
    "# Save (commit) the changes\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34230fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"SQLite JDBC\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/opt/apache-spark/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd()))\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/apache-spark/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd()))\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b317a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>trans</th>\n",
       "      <th>symbol</th>\n",
       "      <th>qty</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>BUY</td>\n",
       "      <td>RHAT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date trans symbol    qty  price\n",
       "0  2006-01-05   BUY   RHAT  100.0  35.14"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "df = ps.read_sql(\"stocks\", con=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549cc2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 17:17:50 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>trans</th>\n",
       "      <th>symbol</th>\n",
       "      <th>qty</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>BUY</td>\n",
       "      <td>RHAT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>BUY</td>\n",
       "      <td>RHAT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date trans symbol    qty  price\n",
       "0  2006-01-05   BUY   RHAT  100.0  35.14\n",
       "1  2006-01-05   BUY   RHAT  100.0  36.14"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.price += 1\n",
    "df.spark.to_spark_io(\n",
    "    format=\"jdbc\", mode=\"append\",\n",
    "    dbtable=\"stocks\", url=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))\n",
    "ps.read_sql(\"stocks\", con=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec64efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
