{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35724761",
   "metadata": {},
   "source": [
    "#Â Spark in Action - Chapter 4 Python Version - Lab 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (lit,col,concat,expr)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8de8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() #os.path.dirname(__file__)\n",
    "relative_path = \"../net.jgp.books.spark.ch04/data/NCHS_-_Teen_Birth_Rates_for_Age_Group_15-19_in_the_United_States_by_County.csv\"\n",
    "absolute_file_path = os.path.join(current_dir, relative_path)\n",
    "\n",
    "mode=\"\"\n",
    "t0 = int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec3e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 19:03:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "1. Creating a session ........... 889\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Creates a session on a local master\n",
    "spark = SparkSession.builder.appName(\"Analysing Catalyst's behavior\") \\\n",
    "    .master(\"local[*]\").getOrCreate()\n",
    "\n",
    "t1 = int(round(time.time() * 1000))\n",
    "\n",
    "print(\"1. Creating a session ........... {}\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ae79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Loading initial dataset ...... 5088\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Reads a CSV file with header, stores it in a dataframe\n",
    "df = spark.read.csv(header=True, inferSchema=True,path=absolute_file_path)\n",
    "\n",
    "initalDf = df\n",
    "t2 = int(round(time.time() * 1000))\n",
    "print(\"2. Loading initial dataset ...... {}\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e988f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Building full dataset ........ 568\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Build a bigger dataset\n",
    "for x in range(60):\n",
    "    df = df.union(initalDf)\n",
    "\n",
    "t3 = int(round(time.time() * 1000))\n",
    "print(\"3. Building full dataset ........ {}\".format(t3 - t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Clean-up ..................... 20\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Cleanup. preparation\n",
    "df = df.withColumnRenamed(\"Lower Confidence Limit\", \"lcl\") \\\n",
    "       .withColumnRenamed(\"Upper Confidence Limit\", \"ucl\")\n",
    "\n",
    "t4 = int(round(time.time() * 1000))\n",
    "print(\"4. Clean-up ..................... {}\".format(t4 - t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd1061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Transformations  ............. 95\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - Transformation\n",
    "if mode.lower != \"noop\":\n",
    "    df =  df.withColumn(\"avg\", expr(\"(lcl+ucl)/2\")) \\\n",
    "            .withColumn(\"lcl2\", col(\"lcl\")) \\\n",
    "            .withColumn(\"ucl2\", col(\"ucl\"))\n",
    "    if mode.lower == \"full\":\n",
    "        df = df.drop(\"avg\",\"lcl2\",\"ucl2\")\n",
    "\n",
    "\n",
    "t5 = int(round(time.time() * 1000))\n",
    "print(\"5. Transformations  ............. {}\".format(t5 - t4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa5903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Final action ................. 24584\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:======================================>                  (41 + 8) / 61]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records .................... 2487641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 6 - Action\n",
    "df.collect()\n",
    "t6 = int(round(time.time() * 1000))\n",
    "print(\"6. Final action ................. {}\".format(t6 - t5))\n",
    "\n",
    "print(\"\")\n",
    "print(\"# of records .................... {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56c259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
