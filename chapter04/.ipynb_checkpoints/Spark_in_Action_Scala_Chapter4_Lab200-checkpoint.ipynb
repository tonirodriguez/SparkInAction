{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1381634",
   "metadata": {},
   "source": [
    "#Â Spark in Action - Chapter 4 Scala Version - Lab 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bba815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.41:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1667059440364)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.util.{Arrays, List}\n",
       "import org.apache.spark.sql.{Dataset, Encoders, SparkSession}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.{Arrays, List}\n",
    "import org.apache.spark.sql.{Dataset, Encoders, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72778ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode: String = noop\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var mode:String = \"noop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a61b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 18:04:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "1. Creating a session ........... 1085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0: Long = 1667059447101\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@26a8c415\n",
       "t1: Long = 1667059448186\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t0 = System.currentTimeMillis\n",
    "\n",
    "// Step 1 - Creates a session on a local maste\n",
    "val spark = SparkSession\n",
    "                .builder\n",
    "                .appName(\"Analysing Catalyst's behavior\")\n",
    "                .master(\"local[*]\")\n",
    "                .getOrCreate\n",
    "\n",
    "val t1 = System.currentTimeMillis\n",
    "println(\"1. Creating a session ........... \" + (t1 - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Loading initial dataset ...... 3887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "initalDf: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "t2: Long = 1667059452073\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 2 - Reads a CSV file with header, stores it in a dataframe\n",
    "var df = spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(\"../net.jgp.books.spark.ch04/data/NCHS_-_Teen_Birth_Rates_for_Age_Group_15-19_in_the_United_States_by_County.csv\")\n",
    "\n",
    "val initalDf = df\n",
    "val t2 = System.currentTimeMillis\n",
    "println(\"2. Loading initial dataset ...... \" + (t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Building full dataset ........ 1117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t3: Long = 1667059453190\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 3 - Build a bigger dataset\n",
    "for(_ <-  0.to(60)){\n",
    "  df = df.union(initalDf)\n",
    "}\n",
    "val t3 = System.currentTimeMillis\n",
    "println(\"3. Building full dataset ........ \" + (t3 - t2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7af2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Clean-up ..................... 241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "t4: Long = 1667059453431\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 4 - Cleanup. preparation\n",
    "df = df.withColumnRenamed(\"Lower Confidence Limit\", \"lcl\")\n",
    "       .withColumnRenamed(\"Upper Confidence Limit\", \"ucl\")\n",
    "\n",
    "val t4 = System.currentTimeMillis\n",
    "println(\"4. Clean-up ..................... \" + (t4 - t3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e851c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Transformations  ............. 258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t5: Long = 1667059453689\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 5 - Transformation\n",
    "if (mode.compareToIgnoreCase(\"noop\") != 0) {\n",
    "  df = df.withColumn(\"avg\", expr(\"(lcl+ucl)/2\"))\n",
    "         .withColumn(\"lcl2\", col(\"lcl\"))\n",
    "         .withColumn(\"ucl2\", col(\"ucl\"))\n",
    "  if (mode.compareToIgnoreCase(\"full\") == 0)\n",
    "    df = df.drop(\"avg\",\"lcl2\",\"ucl2\")\n",
    "}\n",
    "\n",
    "val t5 = System.currentTimeMillis\n",
    "println(\"5. Transformations  ............. \" + (t5 - t4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b3660",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": " GC overhead limit exceeded",
     "output_type": "error",
     "traceback": [
      "java.lang.OutOfMemoryError: GC overhead limit exceeded",
      "  at sun.nio.cs.UTF_8.newDecoder(UTF_8.java:68)",
      "  at java.lang.StringCoding.decode(StringCoding.java:213)",
      "  at java.lang.String.<init>(String.java:463)",
      "  at java.lang.String.<init>(String.java:515)",
      "  at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1355)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.createExternalRow_0_2$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:181)",
      "  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:172)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike$$Lambda$47/1293226111.apply(Unknown Source)",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)",
      "  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)",
      "  at org.apache.spark.sql.Dataset$$Lambda$3739/1921908039.apply(Unknown Source)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)",
      "  at org.apache.spark.sql.Dataset$$Lambda$2744/346838458.apply(Unknown Source)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)",
      "  at org.apache.spark.sql.Dataset$$Lambda$2409/655214864.apply(Unknown Source)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$$$Lambda$2423/1477831658.apply(Unknown Source)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.execution.SQLExecution$$$Lambda$2410/975238827.apply(Unknown Source)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      ""
     ]
    }
   ],
   "source": [
    "// Step 6 - Action\n",
    "df.collect\n",
    "val t6 = System.currentTimeMillis\n",
    "println(\"6. Final action ................. \" + (t6 - t5))\n",
    "\n",
    "println(\"\")\n",
    "println(\"# of records .................... \" + df.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6e8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
