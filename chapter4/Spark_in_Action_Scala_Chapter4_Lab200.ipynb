{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1381634",
   "metadata": {},
   "source": [
    "#Â Spark in Action - Chapter 4 Scala Version - Lab 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bba815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.41:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1667062220119)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.util.{Arrays, List}\n",
       "import org.apache.spark.sql.{Dataset, Encoders, SparkSession}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.{Arrays, List}\n",
    "import org.apache.spark.sql.{Dataset, Encoders, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72778ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode: String = noop\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var mode:String = \"noop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a61b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 18:50:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "1. Creating a session ........... 1061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0: Long = 1667062227021\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6fc94bae\n",
       "t1: Long = 1667062228082\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t0 = System.currentTimeMillis\n",
    "\n",
    "// Step 1 - Creates a session on a local maste\n",
    "val spark = SparkSession\n",
    "                .builder\n",
    "                .appName(\"Analysing Catalyst's behavior\")\n",
    "                .master(\"local[*]\")\n",
    "                .getOrCreate\n",
    "\n",
    "val t1 = System.currentTimeMillis\n",
    "println(\"1. Creating a session ........... \" + (t1 - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Loading initial dataset ...... 3933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "initalDf: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "t2: Long = 1667062232015\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 2 - Reads a CSV file with header, stores it in a dataframe\n",
    "var df = spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(\"../net.jgp.books.spark.ch04/data/NCHS_-_Teen_Birth_Rates_for_Age_Group_15-19_in_the_United_States_by_County.csv\")\n",
    "\n",
    "val initalDf = df\n",
    "val t2 = System.currentTimeMillis\n",
    "println(\"2. Loading initial dataset ...... \" + (t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Building full dataset ........ 1020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t3: Long = 1667062233035\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 3 - Build a bigger dataset\n",
    "for(_ <-  0.to(60)){\n",
    "  df = df.union(initalDf)\n",
    "}\n",
    "val t3 = System.currentTimeMillis\n",
    "println(\"3. Building full dataset ........ \" + (t3 - t2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7af2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Clean-up ..................... 231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "t4: Long = 1667062233266\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 4 - Cleanup. preparation\n",
    "df = df.withColumnRenamed(\"Lower Confidence Limit\", \"lcl\")\n",
    "       .withColumnRenamed(\"Upper Confidence Limit\", \"ucl\")\n",
    "\n",
    "val t4 = System.currentTimeMillis\n",
    "println(\"4. Clean-up ..................... \" + (t4 - t3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e851c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Transformations  ............. 251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t5: Long = 1667062233517\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 5 - Transformation\n",
    "if (mode.compareToIgnoreCase(\"noop\") != 0) {\n",
    "  df = df.withColumn(\"avg\", expr(\"(lcl+ucl)/2\"))\n",
    "         .withColumn(\"lcl2\", col(\"lcl\"))\n",
    "         .withColumn(\"ucl2\", col(\"ucl\"))\n",
    "  if (mode.compareToIgnoreCase(\"full\") == 0)\n",
    "    df = df.drop(\"avg\",\"lcl2\",\"ucl2\")\n",
    "}\n",
    "\n",
    "val t5 = System.currentTimeMillis\n",
    "println(\"5. Transformations  ............. \" + (t5 - t4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b3660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 18:54:06 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.1.41:60730 in 10000 milliseconds\n",
      "22/10/29 18:54:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "// Step 6 - Action\n",
    "df.collect\n",
    "val t6 = System.currentTimeMillis\n",
    "println(\"6. Final action ................. \" + (t6 - t5))\n",
    "\n",
    "println(\"\")\n",
    "println(\"# of records .................... \" + df.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6e8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
