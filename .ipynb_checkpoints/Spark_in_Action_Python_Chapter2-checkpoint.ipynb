{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b64a4e",
   "metadata": {},
   "source": [
    "#Â Spark in Action - Chapter 2 Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18daac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() #os.path.dirname(__file__)\n",
    "relative_path = \"net.jgp.books.spark.ch02/data/authors.csv\"\n",
    "absolute_file_path = os.path.join(current_dir, relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498fa42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/development/ml/Spark/net.jgp.books.spark.ch02/data/authors.csv'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/10/22 00:04:37 INFO SharedState: Warehouse path is 'file:/Users/development/ml/Spark/spark-warehouse'.\n",
      "22/10/22 00:04:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Creates a session on a local master\n",
    "spark = SparkSession.builder.appName(\"CSV to DB\").master(\"local\").config(\"spark.jars\",\"{}/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd())).config(\"spark.driver.extraClassPath\",\"{}/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd())).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:38 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.\n",
      "22/10/22 00:04:38 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "22/10/22 00:04:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/10/22 00:04:41 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "22/10/22 00:04:41 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/10/22 00:04:41 INFO CodeGenerator: Code generated in 210.059255 ms\n",
      "22/10/22 00:04:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 353.3 KiB, free 366.0 MiB)\n",
      "22/10/22 00:04:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 365.9 MiB)\n",
      "22/10/22 00:04:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.41:56749 (size: 34.2 KiB, free: 366.3 MiB)\n",
      "22/10/22 00:04:42 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/10/22 00:04:42 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/22 00:04:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.8 KiB, free 365.9 MiB)\n",
      "22/10/22 00:04:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 365.9 MiB)\n",
      "22/10/22 00:04:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.41:56749 (size: 5.9 KiB, free: 366.3 MiB)\n",
      "22/10/22 00:04:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/10/22 00:04:42 INFO FileScanRDD: Reading File path: file:///Users/development/ml/Spark/net.jgp.books.spark.ch02/data/authors.csv, range: 0-117, partition values: [empty row]\n",
      "22/10/22 00:04:42 INFO CodeGenerator: Code generated in 15.404368 ms\n",
      "22/10/22 00:04:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1533 bytes result sent to driver\n",
      "22/10/22 00:04:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 365 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:42 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0,487 s\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/10/22 00:04:42 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0,531257 s\n",
      "22/10/22 00:04:42 INFO CodeGenerator: Code generated in 12.36932 ms\n",
      "22/10/22 00:04:43 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/10/22 00:04:43 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/10/22 00:04:43 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/10/22 00:04:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.41:56749 in memory (size: 5.9 KiB, free: 366.3 MiB)\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 353.3 KiB, free 365.6 MiB)\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 365.5 MiB)\n",
      "22/10/22 00:04:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.41:56749 (size: 34.2 KiB, free: 366.2 MiB)\n",
      "22/10/22 00:04:43 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/10/22 00:04:43 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 24.9 KiB, free 365.5 MiB)\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.7 KiB, free 365.5 MiB)\n",
      "22/10/22 00:04:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.41:56749 (size: 11.7 KiB, free: 366.2 MiB)\n",
      "22/10/22 00:04:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/10/22 00:04:43 INFO FileScanRDD: Reading File path: file:///Users/development/ml/Spark/net.jgp.books.spark.ch02/data/authors.csv, range: 0-117, partition values: [empty row]\n",
      "22/10/22 00:04:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1513 bytes result sent to driver\n",
      "22/10/22 00:04:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 77 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:43 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0,136 s\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0,143057 s\n"
     ]
    }
   ],
   "source": [
    "#  Step 1: Ingestion\n",
    "#  ---------\n",
    "#\n",
    "#  Reads a CSV file with header, called authors.csv, stores it in a dataframe\n",
    "df = spark.read.csv(header=True, inferSchema=True, path=absolute_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0759ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:43 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/10/22 00:04:43 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/10/22 00:04:43 INFO FileSourceStrategy: Output Data Schema: struct<lname: string, fname: string>\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 353.1 KiB, free 365.2 MiB)\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.1 MiB)\n",
      "22/10/22 00:04:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.41:56749 (size: 34.1 KiB, free: 366.2 MiB)\n",
      "22/10/22 00:04:43 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/10/22 00:04:43 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.3 KiB, free 365.1 MiB)\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 365.1 MiB)\n",
      "22/10/22 00:04:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.41:56749 (size: 5.7 KiB, free: 366.2 MiB)\n",
      "22/10/22 00:04:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "22/10/22 00:04:43 INFO FileScanRDD: Reading File path: file:///Users/development/ml/Spark/net.jgp.books.spark.ch02/data/authors.csv, range: 0-117, partition values: [empty row]\n",
      "22/10/22 00:04:43 INFO CodeGenerator: Code generated in 8.510746 ms\n",
      "22/10/22 00:04:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1588 bytes result sent to driver\n",
      "22/10/22 00:04:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 40 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:43 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0,048 s\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0,052568 s\n",
      "22/10/22 00:04:43 INFO CodeGenerator: Code generated in 10.549799 ms\n",
      "+--------+--------------+\n",
      "|   lname|         fname|\n",
      "+--------+--------------+\n",
      "|  Pascal|        Blaise|\n",
      "|Voltaire|      FranÃ§ois|\n",
      "|  Perrin|  Jean-Georges|\n",
      "|MarÃ©chal|Pierre Sylvain|\n",
      "|   Karau|        Holden|\n",
      "| Zaharia|         Matei|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transform\n",
    "# ---------\n",
    "# Creates a new column called \"name\" as the concatenation of lname, a\n",
    "# virtual column containing \", \" and the fname column\n",
    "df = df.withColumn(\"name\", F.concat(F.col(\"lname\"), F.lit(\", \"), F.col(\"fname\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ea3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:43 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/10/22 00:04:43 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/10/22 00:04:43 INFO FileSourceStrategy: Output Data Schema: struct<lname: string, fname: string>\n",
      "22/10/22 00:04:43 INFO CodeGenerator: Code generated in 16.18374 ms\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 353.1 KiB, free 364.8 MiB)\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.7 MiB)\n",
      "22/10/22 00:04:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.41:56749 (size: 34.1 KiB, free: 366.1 MiB)\n",
      "22/10/22 00:04:43 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/10/22 00:04:43 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.4 KiB, free 364.7 MiB)\n",
      "22/10/22 00:04:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 364.7 MiB)\n",
      "22/10/22 00:04:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.41:56749 (size: 7.2 KiB, free: 366.1 MiB)\n",
      "22/10/22 00:04:43 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:43 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "22/10/22 00:04:43 INFO FileScanRDD: Reading File path: file:///Users/development/ml/Spark/net.jgp.books.spark.ch02/data/authors.csv, range: 0-117, partition values: [empty row]\n",
      "22/10/22 00:04:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1737 bytes result sent to driver\n",
      "22/10/22 00:04:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 17 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:43 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0,024 s\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "22/10/22 00:04:43 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0,028444 s\n",
      "22/10/22 00:04:43 INFO CodeGenerator: Code generated in 11.459573 ms\n",
      "+--------+--------------+--------------------+\n",
      "|   lname|         fname|                name|\n",
      "+--------+--------------+--------------------+\n",
      "|  Pascal|        Blaise|      Pascal, Blaise|\n",
      "|Voltaire|      FranÃ§ois|  Voltaire, FranÃ§ois|\n",
      "|  Perrin|  Jean-Georges|Perrin, Jean-Georges|\n",
      "|MarÃ©chal|Pierre Sylvain|MarÃ©chal, Pierre ...|\n",
      "|   Karau|        Holden|       Karau, Holden|\n",
      "| Zaharia|         Matei|      Zaharia, Matei|\n",
      "+--------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99deed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Save\n",
    "# ----\n",
    "#\n",
    "# The connection URL, assuming your PostgreSQL instance runs locally on the\n",
    "# default port, and the database we use is \"spark_labs\"\n",
    "dbConnectionUrl = \"jdbc:sqlite:/Users/development/ml/Spark/net.jgp.books.spark.ch02/data/spark_labs.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02facd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties to connect to the database, the JDBC driver is part of our pom.xml\n",
    "prop = {\"driver\":\"org.sqlite.JDBC\", \"user\":\"jgp\", \"password\":\"Spark<3Java\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:44 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/10/22 00:04:44 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/10/22 00:04:44 INFO FileSourceStrategy: Output Data Schema: struct<lname: string, fname: string>\n",
      "22/10/22 00:04:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 353.1 KiB, free 364.4 MiB)\n",
      "22/10/22 00:04:44 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.3 MiB)\n",
      "22/10/22 00:04:44 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.41:56749 (size: 34.1 KiB, free: 366.1 MiB)\n",
      "22/10/22 00:04:44 INFO SparkContext: Created broadcast 8 from jdbc at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/10/22 00:04:44 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Got job 4 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Final stage: ResultStage 4 (jdbc at NativeMethodAccessorImpl.java:0)\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[22] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/22 00:04:44 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 30.1 KiB, free 364.3 MiB)\n",
      "22/10/22 00:04:44 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 364.3 MiB)\n",
      "22/10/22 00:04:44 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.41:56749 (size: 14.3 KiB, free: 366.1 MiB)\n",
      "22/10/22 00:04:44 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[22] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:44 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:44 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:44 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "22/10/22 00:04:44 INFO FileScanRDD: Reading File path: file:///Users/development/ml/Spark/net.jgp.books.spark.ch02/data/authors.csv, range: 0-117, partition values: [empty row]\n",
      "22/10/22 00:04:44 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n",
      "22/10/22 00:04:44 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1545 bytes result sent to driver\n",
      "22/10/22 00:04:44 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 43 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:44 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:44 INFO DAGScheduler: ResultStage 4 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0,065 s\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "22/10/22 00:04:44 INFO DAGScheduler: Job 4 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0,069469 s\n"
     ]
    }
   ],
   "source": [
    "# Write in a table called ch02\n",
    "df.write.mode(\"overwrite\").jdbc(url=dbConnectionUrl, table=\"ch02\", properties=prop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e4e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:44 INFO SparkUI: Stopped Spark web UI at http://192.168.1.41:4040\n",
      "22/10/22 00:04:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/10/22 00:04:44 INFO MemoryStore: MemoryStore cleared\n",
      "22/10/22 00:04:44 INFO BlockManager: BlockManager stopped\n",
      "22/10/22 00:04:44 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/10/22 00:04:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/10/22 00:04:44 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# Good to stop SparkSession at the end of the application\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26f01a",
   "metadata": {},
   "source": [
    "##Â MÃ©todo alternativo segÃºn Manual Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c81b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect('example.db')\n",
    "cur = con.cursor()\n",
    "# Create table\n",
    "cur.execute(\n",
    "    '''CREATE TABLE stocks\n",
    "       (date text, trans text, symbol text, qty real, price real)''')\n",
    "# Insert a row of data\n",
    "cur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n",
    "# Save (commit) the changes\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0280db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:44 INFO SparkContext: Running Spark version 3.3.0\n",
      "22/10/22 00:04:44 INFO ResourceUtils: ==============================================================\n",
      "22/10/22 00:04:44 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/10/22 00:04:44 INFO ResourceUtils: ==============================================================\n",
      "22/10/22 00:04:44 INFO SparkContext: Submitted application: SQLite JDBC\n",
      "22/10/22 00:04:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/10/22 00:04:44 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/10/22 00:04:44 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/10/22 00:04:44 INFO SecurityManager: Changing view acls to: toni\n",
      "22/10/22 00:04:44 INFO SecurityManager: Changing modify acls to: toni\n",
      "22/10/22 00:04:44 INFO SecurityManager: Changing view acls groups to: \n",
      "22/10/22 00:04:44 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/10/22 00:04:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(toni); groups with view permissions: Set(); users  with modify permissions: Set(toni); groups with modify permissions: Set()\n",
      "22/10/22 00:04:44 INFO Utils: Successfully started service 'sparkDriver' on port 56751.\n",
      "22/10/22 00:04:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/10/22 00:04:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/10/22 00:04:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/10/22 00:04:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/10/22 00:04:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/10/22 00:04:44 INFO DiskBlockManager: Created local directory at /private/var/folders/xg/f9kxbr7523sc6p79_hjsy2w00000gn/T/blockmgr-9e300745-497e-41e1-b8a9-25d83e8c65a2\n",
      "22/10/22 00:04:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "22/10/22 00:04:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/10/22 00:04:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/10/22 00:04:44 INFO SparkContext: Added JAR /opt/apache-spark/jars/sqlite-jdbc-3.36.0.3.jar at spark://192.168.1.41:56751/jars/sqlite-jdbc-3.36.0.3.jar with timestamp 1666389884648\n",
      "22/10/22 00:04:44 INFO Executor: Starting executor ID driver on host 192.168.1.41\n",
      "22/10/22 00:04:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "22/10/22 00:04:44 INFO Executor: Fetching spark://192.168.1.41:56751/jars/sqlite-jdbc-3.36.0.3.jar with timestamp 1666389884648\n",
      "22/10/22 00:04:44 INFO TransportClientFactory: Successfully created connection to /192.168.1.41:56751 after 32 ms (0 ms spent in bootstraps)\n",
      "22/10/22 00:04:44 INFO Utils: Fetching spark://192.168.1.41:56751/jars/sqlite-jdbc-3.36.0.3.jar to /private/var/folders/xg/f9kxbr7523sc6p79_hjsy2w00000gn/T/spark-31858c5b-1818-42b8-a404-edee0c3b0476/userFiles-18726bba-4601-4f1b-be34-47ad524ce551/fetchFileTemp6967711836480079705.tmp\n",
      "22/10/22 00:04:44 INFO Executor: Adding file:/private/var/folders/xg/f9kxbr7523sc6p79_hjsy2w00000gn/T/spark-31858c5b-1818-42b8-a404-edee0c3b0476/userFiles-18726bba-4601-4f1b-be34-47ad524ce551/sqlite-jdbc-3.36.0.3.jar to class loader\n",
      "22/10/22 00:04:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56753.\n",
      "22/10/22 00:04:44 INFO NettyBlockTransferService: Server created on 192.168.1.41:56753\n",
      "22/10/22 00:04:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/10/22 00:04:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.41, 56753, None)\n",
      "22/10/22 00:04:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.41:56753 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.1.41, 56753, None)\n",
      "22/10/22 00:04:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.41, 56753, None)\n",
      "22/10/22 00:04:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.41, 56753, None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"SQLite JDBC\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/opt/apache-spark/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd()))\n",
    "    .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"/opt/apache-spark/jars/sqlite-jdbc-3.36.0.3.jar\".format(os.getcwd()))\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/10/22 00:04:45 INFO SharedState: Warehouse path is 'file:/Users/development/ml/Spark/spark-warehouse'.\n",
      "22/10/22 00:04:46 INFO CodeGenerator: Code generated in 13.558244 ms\n",
      "22/10/22 00:04:46 INFO SparkContext: Starting job: __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Got job 0 (__repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778) with 1 output partitions\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Final stage: ResultStage 0 (__repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778)\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778), which has no missing parents\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 15.3 KiB, free 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.41:56753 (size: 7.7 KiB, free: 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4409 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/10/22 00:04:46 INFO JDBCRDD: closed connection\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block rdd_2_0 stored as values in memory (estimated size 160.0 B, free 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO BlockManagerInfo: Added rdd_2_0 in memory on 192.168.1.41:56753 (size: 160.0 B, free: 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO CodeGenerator: Code generated in 13.665847 ms\n",
      "22/10/22 00:04:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1473 bytes result sent to driver\n",
      "22/10/22 00:04:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 58 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:46 INFO DAGScheduler: ResultStage 0 (__repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778) finished in 0,072 s\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Job 0 finished: __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778, took 0,080545 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>trans</th>\n",
       "      <th>symbol</th>\n",
       "      <th>qty</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>BUY</td>\n",
       "      <td>RHAT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date trans symbol    qty  price\n",
       "0  2006-01-05   BUY   RHAT  100.0  35.14"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "df = ps.read_sql(\"stocks\", con=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:46 INFO CodeGenerator: Code generated in 14.458876 ms\n",
      "22/10/22 00:04:46 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Final stage: ResultStage 1 (save at NativeMethodAccessorImpl.java:0)\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 27.5 KiB, free 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 366.2 MiB)\n",
      "22/10/22 00:04:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.41:56753 (size: 12.8 KiB, free: 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/10/22 00:04:46 INFO CodeGenerator: Code generated in 14.443146 ms\n",
      "22/10/22 00:04:46 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level 8\n",
      "22/10/22 00:04:46 INFO JDBCRDD: closed connection\n",
      "22/10/22 00:04:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1269 bytes result sent to driver\n",
      "22/10/22 00:04:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 42 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:46 INFO DAGScheduler: ResultStage 1 (save at NativeMethodAccessorImpl.java:0) finished in 0,061 s\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 0,063958 s\n",
      "22/10/22 00:04:46 INFO SparkContext: Starting job: __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Got job 2 (__repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778) with 1 output partitions\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Final stage: ResultStage 2 (__repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778)\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Missing parents: List()\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[17] at __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778), which has no missing parents\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.3 KiB, free 366.2 MiB)\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 366.2 MiB)\n",
      "22/10/22 00:04:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.41:56753 (size: 7.7 KiB, free: 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778) (first 15 tasks are for partitions Vector(0))\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "22/10/22 00:04:46 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.1.41, executor driver, partition 0, PROCESS_LOCAL, 4409 bytes) taskResourceAssignments Map()\n",
      "22/10/22 00:04:46 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "22/10/22 00:04:46 INFO JDBCRDD: closed connection\n",
      "22/10/22 00:04:46 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 296.0 B, free 366.2 MiB)\n",
      "22/10/22 00:04:46 INFO BlockManagerInfo: Added rdd_14_0 in memory on 192.168.1.41:56753 (size: 296.0 B, free: 366.3 MiB)\n",
      "22/10/22 00:04:46 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1490 bytes result sent to driver\n",
      "22/10/22 00:04:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 14 ms on 192.168.1.41 (executor driver) (1/1)\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "22/10/22 00:04:46 INFO DAGScheduler: ResultStage 2 (__repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778) finished in 0,025 s\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/10/22 00:04:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "22/10/22 00:04:46 INFO DAGScheduler: Job 2 finished: __repr__ at /Users/development/mambaforge/envs/fastai/lib/python3.9/site-packages/IPython/lib/pretty.py:778, took 0,030201 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>trans</th>\n",
       "      <th>symbol</th>\n",
       "      <th>qty</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>BUY</td>\n",
       "      <td>RHAT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>BUY</td>\n",
       "      <td>RHAT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date trans symbol    qty  price\n",
       "0  2006-01-05   BUY   RHAT  100.0  35.14\n",
       "1  2006-01-05   BUY   RHAT  100.0  36.14"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.price += 1\n",
    "df.spark.to_spark_io(\n",
    "    format=\"jdbc\", mode=\"append\",\n",
    "    dbtable=\"stocks\", url=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))\n",
    "ps.read_sql(\"stocks\", con=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 00:04:46 INFO SparkUI: Stopped Spark web UI at http://192.168.1.41:4040\n",
      "22/10/22 00:04:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/10/22 00:04:46 INFO MemoryStore: MemoryStore cleared\n",
      "22/10/22 00:04:46 INFO BlockManager: BlockManager stopped\n",
      "22/10/22 00:04:46 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/10/22 00:04:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/10/22 00:04:46 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01f497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
