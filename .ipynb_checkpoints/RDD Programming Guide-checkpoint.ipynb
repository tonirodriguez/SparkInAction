{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81ad775",
   "metadata": {},
   "source": [
    "#Â RDD Programming Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd913665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b39dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/16 21:00:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession from builder\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd507063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Scala\", 25000), (\"Spark\", 35000), (\"PHP\", 21000)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Scala|25000|\n",
      "|Spark|35000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL\n",
    "df.createOrReplaceTempView(\"sample_table\")\n",
    "df2 = spark.sql(\"SELECT _1,_2 FROM sample_table\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0e8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|   _1|   _2|\n",
      "+-----+-----+\n",
      "|Spark|35000|\n",
      "|Scala|25000|\n",
      "|  PHP|21000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Hive table & query it.  \n",
    "spark.table(\"sample_table\").write.saveAsTable(\"sample_hive_table\")\n",
    "df3 = spark.sql(\"SELECT _1,_2 FROM sample_hive_table\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4436f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='Default Hive database', locationUri='file:/Users/development/ml/Spark/spark-warehouse')]\n",
      "[Table(name='sample_hive_table', database='default', description=None, tableType='MANAGED', isTemporary=False), Table(name='sample_table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60918)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/development/mambaforge/envs/fastai/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/development/mambaforge/envs/fastai/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/development/mambaforge/envs/fastai/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/development/mambaforge/envs/fastai/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/apache-spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/apache-spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/opt/apache-spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/apache-spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get metadata from the Catalog\n",
    "# List databases\n",
    "dbs = spark.catalog.listDatabases()\n",
    "print(dbs)\n",
    "\n",
    "# Output\n",
    "#[Database(name='default', description='default database', \n",
    "#locationUri='file:/Users/admin/.spyder-py3/spark-warehouse')]\n",
    "\n",
    "# List Tables\n",
    "tbls = spark.catalog.listTables()\n",
    "print(tbls)\n",
    "\n",
    "#Output\n",
    "#[Table(name='sample_hive_table', database='default', description=None, tableType='MANAGED', #isTemporary=False), Table(name='sample_hive_table1', database='default', description=None, #tableType='MANAGED', isTemporary=False), Table(name='sample_hive_table121', database='default', #description=None, tableType='MANAGED', isTemporary=False), Table(name='sample_table', database=None, #description=None, tableType='TEMPORARY', isTemporary=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386d9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
