{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1381634",
   "metadata": {},
   "source": [
    "#Â Spark in Action - Chapter 4 Scala Version - Lab 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bba815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.41:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1666447188066)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.util.{Arrays, List}\n",
       "import org.apache.spark.sql.{Dataset, Encoders, SparkSession}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.{Arrays, List}\n",
    "import org.apache.spark.sql.{Dataset, Encoders, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccb75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mode: String = noop\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var mode:String = \"noop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a61b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 15:59:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "1. Creating a session ........... 1053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t0: Long = 1666447195136\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@27bbce00\n",
       "t1: Long = 1666447196189\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t0 = System.currentTimeMillis\n",
    "\n",
    "// Step 1 - Creates a session on a local maste\n",
    "val spark = SparkSession\n",
    "                .builder\n",
    "                .appName(\"Analysing Catalyst's behavior\")\n",
    "                .master(\"local[*]\")\n",
    "                .getOrCreate\n",
    "\n",
    "val t1 = System.currentTimeMillis\n",
    "println(\"1. Creating a session ........... \" + (t1 - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Loading initial dataset ...... 3697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "initalDf: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "t2: Long = 1666447199886\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 2 - Reads a CSV file with header, stores it in a dataframe\n",
    "var df = spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(\"net.jgp.books.spark.ch04/data/NCHS_-_Teen_Birth_Rates_for_Age_Group_15-19_in_the_United_States_by_County.csv\")\n",
    "\n",
    "val initalDf = df\n",
    "val t2 = System.currentTimeMillis\n",
    "println(\"2. Loading initial dataset ...... \" + (t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Building full dataset ........ 1146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t3: Long = 1666447201032\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 3 - Build a bigger dataset\n",
    "for(_ <-  0.to(60)){\n",
    "  df = df.union(initalDf)\n",
    "}\n",
    "val t3 = System.currentTimeMillis\n",
    "println(\"3. Building full dataset ........ \" + (t3 - t2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7af2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Clean-up ..................... 299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Year: string, State: string ... 7 more fields]\n",
       "t4: Long = 1666447201331\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 4 - Cleanup. preparation\n",
    "df = df.withColumnRenamed(\"Lower Confidence Limit\", \"lcl\")\n",
    "       .withColumnRenamed(\"Upper Confidence Limit\", \"ucl\")\n",
    "\n",
    "val t4 = System.currentTimeMillis\n",
    "println(\"4. Clean-up ..................... \" + (t4 - t3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e851c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Transformations  ............. 309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t5: Long = 1666447201640\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 5 - Transformation\n",
    "if (mode.compareToIgnoreCase(\"noop\") != 0) {\n",
    "  df = df.withColumn(\"avg\", expr(\"(lcl+ucl)/2\"))\n",
    "         .withColumn(\"lcl2\", col(\"lcl\"))\n",
    "         .withColumn(\"ucl2\", col(\"ucl\"))\n",
    "  if (mode.compareToIgnoreCase(\"full\") == 0)\n",
    "    df = df.drop(\"avg\",\"lcl2\",\"ucl2\")\n",
    "}\n",
    "\n",
    "val t5 = System.currentTimeMillis\n",
    "println(\"5. Transformations  ............. \" + (t5 - t4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b3660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/22 16:00:49 ERROR Utils: Uncaught exception in thread driver-heartbeater\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat scala.runtime.IntRef.create(IntRef.java:23)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.getCurrentMetrics(ExecutorMetrics.scala:101)\n",
      "\tat org.apache.spark.SparkContext.reportHeartBeat(SparkContext.scala:2602)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$26(SparkContext.scala:578)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$581/820078270.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": " GC overhead limit exceeded",
     "output_type": "error",
     "traceback": [
      "java.lang.OutOfMemoryError: GC overhead limit exceeded",
      "  at org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:391)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:402)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:388)",
      "  at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:424)",
      "  at org.apache.spark.sql.execution.SparkPlan$$Lambda$4008/1046740872.apply(Unknown Source)",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)",
      "  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)",
      "  at org.apache.spark.sql.Dataset$$Lambda$3746/948021628.apply(Unknown Source)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)",
      "  at org.apache.spark.sql.Dataset$$Lambda$2752/340989946.apply(Unknown Source)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)",
      "  at org.apache.spark.sql.Dataset$$Lambda$2417/797945068.apply(Unknown Source)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$$$Lambda$2431/2108451172.apply(Unknown Source)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.execution.SQLExecution$$$Lambda$2418/983663645.apply(Unknown Source)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)",
      "  at org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)",
      "  ... 1 elided",
      ""
     ]
    }
   ],
   "source": [
    "// Step 6 - Action\n",
    "df.collect\n",
    "val t6 = System.currentTimeMillis\n",
    "println(\"6. Final action ................. \" + (t6 - t5))\n",
    "\n",
    "println(\"\")\n",
    "println(\"# of records .................... \" + df.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6e8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
