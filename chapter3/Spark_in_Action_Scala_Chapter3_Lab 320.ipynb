{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1381634",
   "metadata": {},
   "source": [
    "# Spark in Action - Chapter 3 Scala Version - Lab 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bba815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.41:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1667057073964)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDate\n",
       "import java.time.format.DateTimeFormatter\n",
       "import org.apache.spark.sql.{functions=>F}\n",
       "import org.apache.spark.sql.{Dataset, Row, Encoders, SparkSession}\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "import org.apache.spark.sql.{Dataset, Row, Encoders, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a61b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 17:24:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6dd36ebe\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creates a session on a local master\n",
    "val spark = SparkSession\n",
    "                .builder\n",
    "                .appName(\"CSV to dataframe to Dataset<Book> and back\")\n",
    "                .master(\"local[*]\")\n",
    "                .getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f247e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Book\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Book(authorId:Int, title:String, releaseDate:LocalDate, link:String, id:Int=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a2715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rowToBook: (row: org.apache.spark.sql.Row)Book\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "* This is a mapper class that will convert a Row to an instance of Book.\n",
    "* You have full control over it - isn't it great that sometimes you have\n",
    "* control?\n",
    "*\n",
    "* @author rambabu.posa\n",
    "*/\n",
    "def rowToBook(row: Row): Book = {\n",
    "    val dateAsString = row.getAs[String](\"releaseDate\")\n",
    "\n",
    "    val releaseDate = if (dateAsString == null) {\n",
    "        LocalDate.parse(\n",
    "          \"01/01/23\",\n",
    "          DateTimeFormatter.ofPattern(\"M/d/yy\")\n",
    "        )\n",
    "    } else {\n",
    "        LocalDate.parse(\n",
    "          dateAsString,\n",
    "          DateTimeFormatter.ofPattern(\"M/d/yy\")\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    val author1 = row.getAs[Int](\"authorId\")\n",
    "    val author2 = if (Option(author1).isDefined) {\n",
    "        author1\n",
    "    } else {\n",
    "        0\n",
    "    }\n",
    "\n",
    "    Book(\n",
    "      author2,\n",
    "      row.getAs[String](\"title\"),\n",
    "      releaseDate,\n",
    "      row.getAs[String](\"link\"),\n",
    "      row.getAs[Int](\"id\")\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36523398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dateAsString: String = 6/17/17\n",
       "releaseDate: java.time.LocalDate = 2017-06-17\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dateAsString = \"6/17/17\" //null\n",
    "\n",
    "val releaseDate = if (dateAsString == null) {\n",
    "    LocalDate.parse(\n",
    "      \"01/01/23\",\n",
    "      DateTimeFormatter.ofPattern(\"M/d/yy\")\n",
    "    )\n",
    "} else {\n",
    "    LocalDate.parse(\n",
    "      dateAsString,\n",
    "      DateTimeFormatter.ofPattern(\"M/d/yy\")\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a1172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author1: Null = null\n",
       "author2: Any = 99\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val author1 = null\n",
    "val author2 = if (Option(author1).isDefined) {\n",
    "    author1\n",
    "} else {\n",
    "    99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7af2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename: String = ../net.jgp.books.spark.ch03/data/books.csv\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filename = \"../net.jgp.books.spark.ch03/data/books.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e851c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: int, authorId: int ... 3 more fields]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b3660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Books ingested in a dataframe\n",
      "+---+--------+--------------------+-----------+--------------------+\n",
      "| id|authorId|               title|releaseDate|                link|\n",
      "+---+--------+--------------------+-----------+--------------------+\n",
      "|  1|       1|Fantastic Beasts ...|   11/18/16|http://amzn.to/2k...|\n",
      "|  2|       1|Harry Potter and ...|    10/6/15|http://amzn.to/2l...|\n",
      "|  3|       1|The Tales of Beed...|    12/4/08|http://amzn.to/2k...|\n",
      "|  4|       1|Harry Potter and ...|    10/4/16|http://amzn.to/2k...|\n",
      "|  5|       2|Informix 12.10 on...|    4/23/17|http://amzn.to/2i...|\n",
      "|  6|       2|Development Tools...|   12/28/16|http://amzn.to/2v...|\n",
      "|  7|       3|Adventures of Huc...|    5/26/94|http://amzn.to/2w...|\n",
      "|  8|       3|A Connecticut Yan...|    6/17/17|http://amzn.to/2x...|\n",
      "| 10|       4|Jacques le Fataliste|     3/1/00|http://amzn.to/2u...|\n",
      "| 11|       4|Diderot Encyclope...|       null|http://amzn.to/2i...|\n",
      "| 12|    null|   A Woman in Berlin|    7/11/06|http://amzn.to/2i...|\n",
      "| 13|       6|Spring Boot in Ac...|     1/3/16|http://amzn.to/2h...|\n",
      "| 14|       6|Spring in Action:...|   11/28/14|http://amzn.to/2y...|\n",
      "| 15|       7|Soft Skills: The ...|   12/29/14|http://amzn.to/2z...|\n",
      "| 16|       8|     Of Mice and Men|       null|http://amzn.to/2z...|\n",
      "| 17|       9|Java 8 in Action:...|    8/28/14|http://amzn.to/2i...|\n",
      "| 18|      12|              Hamlet|     6/8/12|http://amzn.to/2y...|\n",
      "| 19|      13|             Pensées| 12/31/1670|http://amzn.to/2j...|\n",
      "| 20|      14|Fables choisies, ...|   9/1/1999|http://amzn.to/2y...|\n",
      "| 21|      15|Discourse on Meth...|  6/15/1999|http://amzn.to/2h...|\n",
      "| 22|      12|       Twelfth Night|      7/1/4|http://amzn.to/2z...|\n",
      "| 23|      12|             Macbeth|      7/1/3|http://amzn.to/2z...|\n",
      "+---+--------+--------------------+-----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- authorId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- releaseDate: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: Long = 22\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"*** Books ingested in a dataframe\")\n",
    "df.show(30)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22878dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "bookDs: org.apache.spark.sql.Dataset[Book] = [authorId: int, title: string ... 3 more fields]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val bookDs:Dataset[Book] = df.map(rowToBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authorId: integer (nullable = false)\n",
      " |-- title: string (nullable = true)\n",
      " |-- releaseDate: date (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- id: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bookDs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14c76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/29 17:24:50 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 5)\n",
      "java.lang.ClassCastException: $line8.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Book cannot be cast to $line8.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Book\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/10/29 17:24:50 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 5) (192.168.1.41 executor driver): java.lang.ClassCastException: $line8.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Book cannot be cast to $line8.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Book\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/10/29 17:24:50 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 5) (192.168.1.41 executor driver): java.lang.ClassCastException: Book cannot be cast to Book",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 5) (192.168.1.41 executor driver): java.lang.ClassCastException: Book cannot be cast to Book",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:750)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:808)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:767)",
      "  ... 40 elided",
      "Caused by: java.lang.ClassCastException: Book cannot be cast to Book",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:136)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "bookDs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"*** Books are now in a dataset of books\")\n",
    "bookDs.show(5)\n",
    "bookDs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a597883",
   "metadata": {},
   "outputs": [],
   "source": [
    "var df2 = bookDs.toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"releaseDateAsString\",\n",
    "      F.date_format(F.col(\"releaseDate\"), \"M/d/yy\").as(\"MM/dd/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"*** Books are back in a dataframe\")\n",
    "df2.show(5, 13)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4309ab72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
